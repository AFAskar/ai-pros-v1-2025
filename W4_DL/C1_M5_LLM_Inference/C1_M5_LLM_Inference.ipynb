{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "colab-badge",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SDAIA-Bootcamps/ai-pros-v1-2025/blob/main/W4_DL/C1_M5_LLM_Inference/C1_M5_LLM_Inference.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# LLM Inference from Scratch: Demystifying Language Models\n",
    "\n",
    "Welcome! In this notebook, you'll learn that **HuggingFace models are not magic** - they're just PyTorch models with convenient wrappers.\n",
    "\n",
    "We will:\n",
    "1. Load GPT-2 (a small, CPU-friendly language model)\n",
    "2. Explore the **actual PyTorch architecture**\n",
    "3. Perform **manual tokenization** (without using pipelines)\n",
    "4. Implement the **token generation loop from scratch**\n",
    "5. Understand **logits**, **temperature**, and **sampling strategies**\n",
    "\n",
    "By the end, you'll understand exactly what happens inside an LLM during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch transformers -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "NOTEBOOK_NAME = \"C1_M5_LLM_Inference.ipynb\"\n",
    "REPO_URL = \"https://github.com/SDAIA-Bootcamps/ai-pros-v1-2025.git\"\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('Running in Colab')\n",
    "    if os.system('git config --get remote.origin.url') != REPO_URL:\n",
    "        print('Cloning repository...')\n",
    "        !git clone $REPO_URL\n",
    "        repo_dir = REPO_URL.split('/').pop().strip('.git')\n",
    "        print(f'{repo_dir=}')\n",
    "        labdir = next(Path(repo_dir).rglob(NOTEBOOK_NAME)).parent.absolute()\n",
    "        print(f'{labdir=}')\n",
    "        %cd $labdir\n",
    "\n",
    "%matplotlib inline\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-section",
   "metadata": {},
   "source": [
    "## Part 1: The Imports - What Are We Actually Loading?\n",
    "\n",
    "Let's start with the imports and understand what each one does:\n",
    "\n",
    "- **`torch`**: The PyTorch library - the actual deep learning framework\n",
    "- **`torch.nn.functional`**: Contains functions like softmax that we'll use for sampling\n",
    "- **`GPT2LMHeadModel`**: This is just a PyTorch `nn.Module` subclass!\n",
    "- **`GPT2Tokenizer`**: Converts text to numbers and back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loading-model-section",
   "metadata": {},
   "source": [
    "## Part 2: Loading the Model - It's Just PyTorch!\n",
    "\n",
    "When we load a HuggingFace model, we're downloading:\n",
    "1. **Model weights**: The learned parameters (numbers)\n",
    "2. **Model config**: Architecture details (how many layers, dimensions, etc.)\n",
    "3. **Tokenizer files**: Vocabulary and encoding rules\n",
    "\n",
    "The model itself is a standard PyTorch `nn.Module`. Let's prove it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = \"gpt2\"  # This is the smallest GPT-2 variant (~124M parameters)\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode (disables dropout)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL LOADED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prove-pytorch",
   "metadata": {},
   "source": [
    "### Proof: It's a PyTorch nn.Module\n",
    "\n",
    "Let's verify that this HuggingFace model is indeed a standard PyTorch module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-pytorch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify it's a PyTorch module\n",
    "print(f\"Is the model an nn.Module? {isinstance(model, torch.nn.Module)}\")\n",
    "print(f\"Model class: {type(model)}\")\n",
    "print(f\"Parent classes: {type(model).__bases__}\")\n",
    "\n",
    "# Count parameters (just like any PyTorch model)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture-section",
   "metadata": {},
   "source": [
    "## Part 3: Exploring the Architecture\n",
    "\n",
    "GPT-2 is a **decoder-only transformer**. Let's look at its structure:\n",
    "\n",
    "```\n",
    "GPT2LMHeadModel\n",
    "├── transformer (GPT2Model)\n",
    "│   ├── wte: Word Token Embeddings (vocab_size x embed_dim)\n",
    "│   ├── wpe: Word Position Embeddings (max_positions x embed_dim)\n",
    "│   ├── h: ModuleList of transformer blocks\n",
    "│   │   └── [0-11]: GPT2Block (12 identical blocks)\n",
    "│   │       ├── ln_1: LayerNorm\n",
    "│   │       ├── attn: GPT2Attention (self-attention)\n",
    "│   │       ├── ln_2: LayerNorm\n",
    "│   │       └── mlp: GPT2MLP (feed-forward network)\n",
    "│   └── ln_f: Final LayerNorm\n",
    "└── lm_head: Linear layer (embed_dim -> vocab_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "print-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the full architecture\n",
    "print(\"FULL MODEL ARCHITECTURE:\")\n",
    "print(\"=\"*60)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the model configuration\n",
    "config = model.config\n",
    "\n",
    "print(\"MODEL CONFIGURATION:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Vocabulary size: {config.vocab_size:,} tokens\")\n",
    "print(f\"Maximum sequence length: {config.n_positions} tokens\")\n",
    "print(f\"Embedding dimension: {config.n_embd}\")\n",
    "print(f\"Number of attention heads: {config.n_head}\")\n",
    "print(f\"Number of transformer layers: {config.n_layer}\")\n",
    "print(f\"Feed-forward hidden size: {config.n_inner if config.n_inner else config.n_embd * 4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-layers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect individual layers\n",
    "print(\"\\nKEY LAYER DIMENSIONS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Token embeddings\n",
    "wte = model.transformer.wte\n",
    "print(f\"Token Embedding (wte): {wte.weight.shape}\")\n",
    "print(f\"  -> Maps {wte.weight.shape[0]:,} vocabulary tokens to {wte.weight.shape[1]}-dimensional vectors\")\n",
    "\n",
    "# Position embeddings\n",
    "wpe = model.transformer.wpe\n",
    "print(f\"\\nPosition Embedding (wpe): {wpe.weight.shape}\")\n",
    "print(f\"  -> Maps {wpe.weight.shape[0]} positions to {wpe.weight.shape[1]}-dimensional vectors\")\n",
    "\n",
    "# First transformer block\n",
    "block = model.transformer.h[0]\n",
    "print(f\"\\nFirst Transformer Block:\")\n",
    "print(f\"  Attention Q,K,V projection: {block.attn.c_attn.weight.shape}\")\n",
    "print(f\"  Attention output projection: {block.attn.c_proj.weight.shape}\")\n",
    "print(f\"  MLP first layer: {block.mlp.c_fc.weight.shape}\")\n",
    "print(f\"  MLP second layer: {block.mlp.c_proj.weight.shape}\")\n",
    "\n",
    "# LM head\n",
    "print(f\"\\nLanguage Model Head (lm_head): {model.lm_head.weight.shape}\")\n",
    "print(f\"  -> Projects {model.lm_head.weight.shape[1]}-dimensional hidden states to {model.lm_head.weight.shape[0]:,} vocabulary logits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenization-section",
   "metadata": {},
   "source": [
    "## Part 4: Tokenization - Converting Text to Numbers\n",
    "\n",
    "Before the model can process text, we need to convert it to numbers. This is called **tokenization**.\n",
    "\n",
    "GPT-2 uses **Byte Pair Encoding (BPE)**, which:\n",
    "- Breaks words into subword units\n",
    "- Can handle any text (no unknown tokens)\n",
    "- Balances vocabulary size with sequence length\n",
    "\n",
    "Let's see exactly how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenization-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text to tokenize\n",
    "text = \"Hello, I am a language model.\"\n",
    "\n",
    "print(\"TOKENIZATION PROCESS:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Input text: '{text}'\")\n",
    "print()\n",
    "\n",
    "# Step 1: Encode text to token IDs\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(f\"Step 1 - Token IDs: {token_ids}\")\n",
    "print(f\"         Number of tokens: {len(token_ids)}\")\n",
    "print()\n",
    "\n",
    "# Step 2: See what each token represents\n",
    "print(\"Step 2 - Token breakdown:\")\n",
    "for i, token_id in enumerate(token_ids):\n",
    "    token_text = tokenizer.decode([token_id])\n",
    "    print(f\"  Position {i}: ID={token_id:5d} -> '{token_text}'\")\n",
    "print()\n",
    "\n",
    "# Step 3: Convert to PyTorch tensor\n",
    "input_ids = torch.tensor([token_ids]).to(device)\n",
    "print(f\"Step 3 - PyTorch tensor shape: {input_ids.shape}\")\n",
    "print(f\"         Tensor: {input_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenization-examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how different words get tokenized\n",
    "examples = [\n",
    "    \"cat\",\n",
    "    \"concatenate\",\n",
    "    \"artificial intelligence\",\n",
    "    \"GPT-2\",\n",
    "    \"transformer\",\n",
    "    \"antidisestablishmentarianism\",\n",
    "]\n",
    "\n",
    "print(\"TOKENIZATION EXAMPLES:\")\n",
    "print(\"=\"*60)\n",
    "for word in examples:\n",
    "    tokens = tokenizer.encode(word)\n",
    "    token_strs = [tokenizer.decode([t]) for t in tokens]\n",
    "    print(f\"'{word}'\")\n",
    "    print(f\"  -> IDs: {tokens}\")\n",
    "    print(f\"  -> Tokens: {token_strs}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-pass-section",
   "metadata": {},
   "source": [
    "## Part 5: The Forward Pass - Getting Logits\n",
    "\n",
    "Now let's run the model and see exactly what comes out. The output is **logits** - raw scores for each possible next token.\n",
    "\n",
    "**Key concept**: The model outputs a score for EVERY token in the vocabulary for EACH position in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-pass",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input\n",
    "prompt = \"The capital of France is\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "print(\"FORWARD PASS:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Input prompt: '{prompt}'\")\n",
    "print(f\"Input shape: {input_ids.shape}  (batch_size=1, sequence_length={input_ids.shape[1]})\")\n",
    "print()\n",
    "\n",
    "# Run the forward pass (no gradients needed for inference)\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "# The output contains logits\n",
    "logits = outputs.logits\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "print(f\"  -> (batch_size={logits.shape[0]}, sequence_length={logits.shape[1]}, vocab_size={logits.shape[2]})\")\n",
    "print()\n",
    "print(\"This means: for each of the {} input tokens, we get {} scores (one per vocabulary token)\".format(\n",
    "    logits.shape[1], logits.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "examine-logits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the logits for the LAST position (predicting the next token)\n",
    "last_token_logits = logits[0, -1, :]  # Shape: (vocab_size,)\n",
    "\n",
    "print(\"EXAMINING LOGITS FOR NEXT TOKEN PREDICTION:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Logits for last position: shape {last_token_logits.shape}\")\n",
    "print(f\"Min logit: {last_token_logits.min().item():.4f}\")\n",
    "print(f\"Max logit: {last_token_logits.max().item():.4f}\")\n",
    "print(f\"Mean logit: {last_token_logits.mean().item():.4f}\")\n",
    "print()\n",
    "\n",
    "# Find the top predictions\n",
    "top_k = 10\n",
    "top_logits, top_indices = torch.topk(last_token_logits, top_k)\n",
    "\n",
    "print(f\"Top {top_k} predictions:\")\n",
    "print(\"-\" * 40)\n",
    "for i, (logit, idx) in enumerate(zip(top_logits, top_indices)):\n",
    "    token = tokenizer.decode([idx.item()])\n",
    "    print(f\"  {i+1}. '{token}' (ID: {idx.item()}, logit: {logit.item():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "softmax-section",
   "metadata": {},
   "source": [
    "## Part 6: From Logits to Probabilities - The Softmax Function\n",
    "\n",
    "Logits are raw scores - they can be any number. To convert them to probabilities (values between 0 and 1 that sum to 1), we use **softmax**:\n",
    "\n",
    "$$P(token_i) = \\frac{e^{logit_i}}{\\sum_j e^{logit_j}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "softmax-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert logits to probabilities using softmax\n",
    "probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "print(\"SOFTMAX: CONVERTING LOGITS TO PROBABILITIES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Probabilities shape: {probabilities.shape}\")\n",
    "print(f\"Sum of all probabilities: {probabilities.sum().item():.6f}\")\n",
    "print(f\"Min probability: {probabilities.min().item():.2e}\")\n",
    "print(f\"Max probability: {probabilities.max().item():.4f}\")\n",
    "print()\n",
    "\n",
    "# Show top predictions with probabilities\n",
    "top_probs, top_indices = torch.topk(probabilities, top_k)\n",
    "\n",
    "print(f\"Top {top_k} predictions with probabilities:\")\n",
    "print(\"-\" * 50)\n",
    "for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "    token = tokenizer.decode([idx.item()])\n",
    "    logit = last_token_logits[idx].item()\n",
    "    print(f\"  {i+1}. '{token}' -> logit: {logit:7.3f} -> prob: {prob.item():.4f} ({prob.item()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temperature-section",
   "metadata": {},
   "source": [
    "## Part 7: Temperature - Controlling Randomness\n",
    "\n",
    "**Temperature** is a parameter that controls how \"confident\" or \"creative\" the model is:\n",
    "\n",
    "$$P(token_i) = \\frac{e^{logit_i / T}}{\\sum_j e^{logit_j / T}}$$\n",
    "\n",
    "- **T = 1.0**: Normal behavior (default)\n",
    "- **T < 1.0**: More confident (sharper distribution, more deterministic)\n",
    "- **T > 1.0**: More creative (flatter distribution, more random)\n",
    "- **T → 0**: Always picks the highest probability token (greedy)\n",
    "- **T → ∞**: Uniform distribution (completely random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temperature-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def apply_temperature(logits, temperature):\n",
    "    \"\"\"Apply temperature scaling to logits.\"\"\"\n",
    "    return F.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "# Test different temperatures\n",
    "temperatures = [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "\n",
    "print(\"TEMPERATURE EFFECT ON PROBABILITY DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Temperature':<12} | {'Top 1 prob':<12} | {'Top 5 sum':<12} | Top prediction\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for temp in temperatures:\n",
    "    probs = apply_temperature(last_token_logits, temp)\n",
    "    top_prob = probs.max().item()\n",
    "    top_5_sum = probs.topk(5).values.sum().item()\n",
    "    top_token = tokenizer.decode([probs.argmax().item()])\n",
    "    print(f\"{temp:<12.1f} | {top_prob:<12.4f} | {top_5_sum:<12.4f} | '{top_token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temperature-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature effects\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, temp in zip(axes, [0.3, 1.0, 2.0]):\n",
    "    probs = apply_temperature(last_token_logits, temp)\n",
    "    top_probs, top_indices = probs.topk(15)\n",
    "    tokens = [tokenizer.decode([idx.item()]).strip() for idx in top_indices]\n",
    "    \n",
    "    ax.barh(range(len(tokens)), top_probs.cpu().numpy())\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_yticklabels(tokens)\n",
    "    ax.set_xlabel('Probability')\n",
    "    ax.set_title(f'Temperature = {temp}')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f'Prompt: \"{prompt}\" + ?', y=1.02, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sampling-section",
   "metadata": {},
   "source": [
    "## Part 8: Sampling Strategies\n",
    "\n",
    "Once we have probabilities, we need to select the next token. There are several strategies:\n",
    "\n",
    "1. **Greedy**: Always pick the highest probability token\n",
    "2. **Random Sampling**: Sample according to the probability distribution\n",
    "3. **Top-K Sampling**: Only consider the top K tokens\n",
    "4. **Top-P (Nucleus) Sampling**: Only consider tokens until cumulative probability reaches P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sampling-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_sample(logits):\n",
    "    \"\"\"Select the token with highest probability.\"\"\"\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "def random_sample(logits, temperature=1.0):\n",
    "    \"\"\"Sample from the probability distribution.\"\"\"\n",
    "    probs = F.softmax(logits / temperature, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "\n",
    "def top_k_sample(logits, k=50, temperature=1.0):\n",
    "    \"\"\"Sample from the top-k tokens only.\"\"\"\n",
    "    # Get top-k logits\n",
    "    top_k_logits, top_k_indices = logits.topk(k)\n",
    "    # Apply temperature and softmax\n",
    "    probs = F.softmax(top_k_logits / temperature, dim=-1)\n",
    "    # Sample from top-k\n",
    "    sampled_idx = torch.multinomial(probs, num_samples=1)\n",
    "    # Map back to original vocabulary index\n",
    "    return top_k_indices.gather(-1, sampled_idx).squeeze(-1)\n",
    "\n",
    "def top_p_sample(logits, p=0.9, temperature=1.0):\n",
    "    \"\"\"Sample using nucleus (top-p) sampling.\"\"\"\n",
    "    # Apply temperature\n",
    "    probs = F.softmax(logits / temperature, dim=-1)\n",
    "    # Sort probabilities in descending order\n",
    "    sorted_probs, sorted_indices = probs.sort(descending=True)\n",
    "    # Calculate cumulative probabilities\n",
    "    cumulative_probs = sorted_probs.cumsum(dim=-1)\n",
    "    # Find where cumulative probability exceeds p\n",
    "    mask = cumulative_probs <= p\n",
    "    # Always include at least one token\n",
    "    mask[..., 0] = True\n",
    "    # Zero out probabilities for tokens outside nucleus\n",
    "    sorted_probs = sorted_probs * mask.float()\n",
    "    # Renormalize\n",
    "    sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "    # Sample\n",
    "    sampled_idx = torch.multinomial(sorted_probs, num_samples=1)\n",
    "    # Map back to original vocabulary index\n",
    "    return sorted_indices.gather(-1, sampled_idx).squeeze(-1)\n",
    "\n",
    "print(\"Sampling functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sampling-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate each sampling method\n",
    "print(\"SAMPLING STRATEGIES DEMO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print()\n",
    "\n",
    "# Greedy\n",
    "greedy_token = greedy_sample(last_token_logits)\n",
    "print(f\"Greedy sampling: '{tokenizer.decode([greedy_token.item()])}'\")\n",
    "\n",
    "# Random sampling (run multiple times to see variation)\n",
    "print(\"\\nRandom sampling (10 samples):\")\n",
    "for i in range(10):\n",
    "    token = random_sample(last_token_logits, temperature=1.0)\n",
    "    print(f\"  {i+1}. '{tokenizer.decode([token.item()])}'\")\n",
    "\n",
    "# Top-K sampling\n",
    "print(\"\\nTop-K sampling (k=5, 10 samples):\")\n",
    "for i in range(10):\n",
    "    token = top_k_sample(last_token_logits, k=5, temperature=1.0)\n",
    "    print(f\"  {i+1}. '{tokenizer.decode([token.item()])}'\")\n",
    "\n",
    "# Top-P sampling\n",
    "print(\"\\nTop-P sampling (p=0.9, 10 samples):\")\n",
    "for i in range(10):\n",
    "    token = top_p_sample(last_token_logits, p=0.9, temperature=1.0)\n",
    "    print(f\"  {i+1}. '{tokenizer.decode([token.item()])}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generation-loop-section",
   "metadata": {},
   "source": [
    "## Part 9: The Token Generation Loop - Putting It All Together\n",
    "\n",
    "Now we'll implement the complete text generation loop from scratch. This is what happens inside `model.generate()` or HuggingFace pipelines:\n",
    "\n",
    "```\n",
    "1. Tokenize input prompt\n",
    "2. LOOP:\n",
    "   a. Forward pass through model -> get logits\n",
    "   b. Get logits for last position\n",
    "   c. Apply temperature\n",
    "   d. Sample next token\n",
    "   e. Append to sequence\n",
    "   f. Check stopping conditions\n",
    "3. Decode tokens back to text\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generation-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=50,\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    top_p=None,\n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text using the model, token by token.\n",
    "    \n",
    "    Args:\n",
    "        model: The GPT-2 model\n",
    "        tokenizer: The tokenizer\n",
    "        prompt: Starting text\n",
    "        max_new_tokens: Maximum number of tokens to generate\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "        top_k: If set, only sample from top-k tokens\n",
    "        top_p: If set, use nucleus sampling with this threshold\n",
    "        verbose: If True, print each token as it's generated\n",
    "    \n",
    "    Returns:\n",
    "        Generated text string\n",
    "    \"\"\"\n",
    "    # Step 1: Tokenize the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Starting generation...\")\n",
    "        print(f\"Initial sequence length: {input_ids.shape[1]} tokens\")\n",
    "        print(f\"Prompt: '{prompt}'\")\n",
    "        print(\"\\nGenerating:\", end=\" \")\n",
    "    \n",
    "    # Step 2: Generation loop\n",
    "    generated_tokens = []\n",
    "    \n",
    "    for step in range(max_new_tokens):\n",
    "        # Step 2a: Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        # Step 2b: Get logits for the last position\n",
    "        next_token_logits = logits[0, -1, :]\n",
    "        \n",
    "        # Step 2c & 2d: Apply temperature and sample\n",
    "        if top_k is not None:\n",
    "            next_token = top_k_sample(next_token_logits, k=top_k, temperature=temperature)\n",
    "        elif top_p is not None:\n",
    "            next_token = top_p_sample(next_token_logits, p=top_p, temperature=temperature)\n",
    "        elif temperature == 0:\n",
    "            next_token = greedy_sample(next_token_logits)\n",
    "        else:\n",
    "            next_token = random_sample(next_token_logits, temperature=temperature)\n",
    "        \n",
    "        # Step 2e: Append to sequence\n",
    "        next_token = next_token.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        \n",
    "        generated_tokens.append(next_token.item())\n",
    "        \n",
    "        if verbose:\n",
    "            token_text = tokenizer.decode([next_token.item()])\n",
    "            print(token_text, end=\"\", flush=True)\n",
    "        \n",
    "        # Step 2f: Check stopping condition (EOS token)\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            if verbose:\n",
    "                print(\"\\n[EOS token reached]\")\n",
    "            break\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n\\nGenerated {len(generated_tokens)} new tokens\")\n",
    "    \n",
    "    # Step 3: Decode full sequence\n",
    "    full_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return full_text\n",
    "\n",
    "print(\"Generation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generation-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the generation function\n",
    "prompt = \"Once upon a time, in a land far away,\"\n",
    "\n",
    "print(\"TEXT GENERATION DEMO\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Generate with verbose output to see the process\n",
    "generated = generate_text(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.8,\n",
    "    top_k=50,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL OUTPUT:\")\n",
    "print(\"=\"*60)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-settings",
   "metadata": {},
   "source": [
    "## Part 10: Comparing Different Generation Settings\n",
    "\n",
    "Let's see how different settings affect the generated text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-temperatures",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "print(\"COMPARING TEMPERATURE SETTINGS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print()\n",
    "\n",
    "for temp in [0.3, 0.7, 1.0, 1.5]:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Temperature = {temp}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    torch.manual_seed(42)  # Reset seed for fair comparison\n",
    "    text = generate_text(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=40,\n",
    "        temperature=temp,\n",
    "        top_p=0.9,\n",
    "        verbose=False\n",
    "    )\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Scientists have discovered that\"\n",
    "\n",
    "print(\"COMPARING SAMPLING STRATEGIES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "\n",
    "settings = [\n",
    "    {\"name\": \"Greedy (temp=0)\", \"temperature\": 0, \"top_k\": None, \"top_p\": None},\n",
    "    {\"name\": \"Top-K (k=10)\", \"temperature\": 0.8, \"top_k\": 10, \"top_p\": None},\n",
    "    {\"name\": \"Top-K (k=50)\", \"temperature\": 0.8, \"top_k\": 50, \"top_p\": None},\n",
    "    {\"name\": \"Top-P (p=0.5)\", \"temperature\": 0.8, \"top_k\": None, \"top_p\": 0.5},\n",
    "    {\"name\": \"Top-P (p=0.95)\", \"temperature\": 0.8, \"top_k\": None, \"top_p\": 0.95},\n",
    "]\n",
    "\n",
    "for setting in settings:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{setting['name']}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    text = generate_text(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=40,\n",
    "        temperature=setting['temperature'],\n",
    "        top_k=setting['top_k'],\n",
    "        top_p=setting['top_p'],\n",
    "        verbose=False\n",
    "    )\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-by-step-section",
   "metadata": {},
   "source": [
    "## Part 11: Detailed Step-by-Step Visualization\n",
    "\n",
    "Let's generate a few tokens and visualize exactly what happens at each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step-by-step",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_details(model, tokenizer, prompt, num_tokens=5, temperature=1.0):\n",
    "    \"\"\"Generate tokens and show detailed information at each step.\"\"\"\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    print(\"STEP-BY-STEP GENERATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Tokenized: {input_ids[0].tolist()}\")\n",
    "    print()\n",
    "    \n",
    "    for step in range(num_tokens):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"STEP {step + 1}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Current sequence\n",
    "        current_text = tokenizer.decode(input_ids[0])\n",
    "        print(f\"Current sequence: '{current_text}'\")\n",
    "        print(f\"Sequence length: {input_ids.shape[1]} tokens\")\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        # Apply temperature and get probabilities\n",
    "        probs = F.softmax(logits / temperature, dim=-1)\n",
    "        \n",
    "        # Show top candidates\n",
    "        print(f\"\\nTop 5 candidates (temperature={temperature}):\")\n",
    "        top_probs, top_indices = probs.topk(5)\n",
    "        for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "            token_text = tokenizer.decode([idx.item()])\n",
    "            logit = logits[idx].item()\n",
    "            print(f\"  {i+1}. '{token_text}' | logit: {logit:7.2f} | prob: {prob.item():.4f} ({prob.item()*100:.1f}%)\")\n",
    "        \n",
    "        # Sample next token\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        next_token_text = tokenizer.decode([next_token.item()])\n",
    "        print(f\"\\n>>> Sampled token: '{next_token_text}' (ID: {next_token.item()})\")\n",
    "        \n",
    "        # Append\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    final_text = tokenizer.decode(input_ids[0])\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FINAL RESULT: '{final_text}'\")\n",
    "    return final_text\n",
    "\n",
    "# Run the detailed generation\n",
    "torch.manual_seed(123)\n",
    "result = generate_with_details(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=\"The secret to happiness is\",\n",
    "    num_tokens=5,\n",
    "    temperature=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "## Summary: Key Takeaways\n",
    "\n",
    "### 1. HuggingFace Models Are Just PyTorch\n",
    "- `GPT2LMHeadModel` is a subclass of `torch.nn.Module`\n",
    "- You can inspect parameters, count them, and access layers like any PyTorch model\n",
    "- The \"magic\" is just well-organized code\n",
    "\n",
    "### 2. The Architecture\n",
    "- **Token Embeddings**: Convert token IDs to vectors\n",
    "- **Position Embeddings**: Add position information\n",
    "- **Transformer Blocks**: Self-attention + feed-forward networks\n",
    "- **LM Head**: Project hidden states to vocabulary logits\n",
    "\n",
    "### 3. Tokenization\n",
    "- Converts text to sequences of integers\n",
    "- GPT-2 uses Byte Pair Encoding (BPE)\n",
    "- Words may be split into multiple subword tokens\n",
    "\n",
    "### 4. Logits and Probabilities\n",
    "- Model outputs raw logits (unnormalized scores) for each vocabulary token\n",
    "- Softmax converts logits to probabilities\n",
    "- Temperature scales logits before softmax to control randomness\n",
    "\n",
    "### 5. Sampling Strategies\n",
    "- **Greedy**: Always pick highest probability (deterministic)\n",
    "- **Random**: Sample from full distribution\n",
    "- **Top-K**: Only consider K highest probability tokens\n",
    "- **Top-P (Nucleus)**: Only consider tokens until cumulative probability reaches P\n",
    "\n",
    "### 6. The Generation Loop\n",
    "```\n",
    "For each new token:\n",
    "  1. Forward pass → get logits\n",
    "  2. Apply temperature (optional)\n",
    "  3. Sample next token\n",
    "  4. Append to sequence\n",
    "  5. Repeat until stopping condition\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Try these exercises to deepen your understanding:\n",
    "\n",
    "1. **Modify the generation function** to implement a `repetition_penalty` that reduces the probability of tokens that have already appeared.\n",
    "\n",
    "2. **Visualize attention patterns**: Access `outputs.attentions` (with `output_attentions=True`) and plot which tokens attend to which.\n",
    "\n",
    "3. **Compare models**: Load `gpt2-medium` or `gpt2-large` and compare the quality of generation.\n",
    "\n",
    "4. **Implement beam search**: Instead of sampling one token at a time, track multiple candidate sequences and choose the best overall.\n",
    "\n",
    "5. **Add a stopping criteria**: Stop generation when the model produces a period or newline character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for your experiments!\n",
    "# Try modifying the generation parameters or implementing the exercises above.\n",
    "\n",
    "prompt = \"In the year 2050,\"\n",
    "\n",
    "# Your code here...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
